import random
from tqdm.auto import tqdm
import numpy as np
import pandas as pd
import torch
from transformers import DataCollatorWithPadding
from termcolor import colored


class Ansi:
    green = '\033[32m'
    red = '\033[31m'
    bold = '\033[1m'
    underline = '\033[4m'
    end = '\033[0m'


def df_info(df: pd.DataFrame, isna_cols: list | None = None):
    display(df.head())
    print(f"{Ansi.bold}Shape:{Ansi.end} {df.shape}")
    print(f"\n{Ansi.bold}Missing values:{Ansi.end}")

    if isna_cols is not None:
        df = df[isna_cols]

    print(df.isna().sum().to_string(index=True))


def get_rare_labels_from_multihot(labels: np.ndarray) -> np.ndarray:
    """
    –§—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –º–∞—Å—Å–∏–≤ multi-hot –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑, 
    –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—Ç–∏–ª—Å—è –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Å –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∫–∞–∂–¥–æ–º—É –æ–±—ä–µ–∫—Ç—É –∏–Ω–¥–µ–∫—Å —Å–∞–º–æ–≥–æ 
    –º–∞–ª–æ—á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –∏–∑ –∏–º–µ—é—â–∏—Ö—Å—è –∫–ª–∞—Å—Å–æ–≤.
    –ü–æ–ª—É—á–µ–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –¥–ª—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –Ω–∞ –≤—ã–±–æ—Ä–∫–∏.
    """
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Å
    labels_cnt = np.sum(labels, axis=0)
    
    rare_labels = []
    for label in labels:
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É
        masked = labels_cnt[label == 1]
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–º –º–∞—Å—Å–∏–≤–µ
        min_index_masked = np.argmin(masked)
        min_index = np.where(label == 1)[0][min_index_masked]
        rare_labels.append(min_index)

    return np.array(rare_labels)


#------------------
# Torch
#------------------
def torch_device() -> str:
    """–í–æ–≤–∑—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É 'cuda' –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è, –∏–Ω–∞—á–µ 'cpu'."""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"torch use: {colored(device, 'green', attrs=['bold'])}", 
          f"({torch.cuda.get_device_name()})"
          if torch.cuda.is_available() else "")
    return device


def get_model_size(model) -> str:
    model_size = sum(p.numel() for p in model.parameters()) / 1e6
    model_size_train = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6
    print(f"Model_size: {Ansi.bold}{model_size:.2f}{Ansi.end}M params", end='')

    if model_size_train < model_size:
        print(f" || Trainable params: {Ansi.bold}{model_size_train:.2f}{Ansi.end}M "
              f"({Ansi.bold}{model_size_train / model_size:.4%}{Ansi.end})")


def get_requires_grad(model, detail=True, only_true=False) -> str:
    params = {name: p.requires_grad for name, p in model.named_parameters()}
    params = pd.Series(params)

    get_model_size(model)

    print(f"\n{Ansi.bold}Requires grad:{Ansi.end}")
    print(f"True:  {Ansi.bold}{params.sum()}{Ansi.end}")
    print(f"False: {Ansi.bold}{(~params).sum()}{Ansi.end}")

    if detail:
        print()
        if only_true:
            print(params[params].to_string(index=True))
        else:
            print(params.to_string(index=True))


#------------------
# transformers
#------------------
def check_unk_tokens(texts: list[str], tokenizer, mode='chars') -> list[str]:
    
    def get_unk_words(text) -> list[str]:
        tokenize = tokenizer(text, return_offsets_mapping=True, return_token_type_ids=False, 
                             return_attention_mask=False, return_tensors='pt')
        input_ids = tokenize['input_ids'][0]
        offsets_map = tokenize['offset_mapping'][0]

        idx_unk_token_in_text = np.where(input_ids == unk_token_id)[0]
        slices_unk_token = offsets_map[[idx_unk_token_in_text]]
        return [text[start:end] for start, end in slices_unk_token]

    unk_token_id = tokenizer.unk_token_id

    # –ü–æ–ª—É—á–∞–µ–º –º–∞—Å—Å–∏–≤ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤, –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ—Ç–æ—Ä—ã—Ö –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π 
    # —Ç–æ–∫–µ–Ω [UNK]. –í –¥–∞–Ω–Ω–æ–º –º–∞—Å—Å–∏–≤–µ –º–æ–≥—É—Ç –æ—Å—Ç–∞—Ç—å—Å—è –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∫ –∫–æ—Ç–æ—Ä—ã–º –±–µ–∑ –ø—Ä–æ–±–µ–ª–∞ 
    # –¥–æ–±–∞–≤–ª–µ–Ω –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª, –Ω–∞–ø—Ä–∏–º–µ—Ä "–û–∫üëå" - –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –Ω–µ –æ–±—É—á–µ–Ω –Ω–∞ emoji.
    unk_words = set()
    for text in tqdm(texts):
        unk_words.update(get_unk_words(text))

    if mode =='words':
        return unk_words

    # –ü–æ–ª—É—á–∏–≤—à–∏–π—Å—è –º–∞—Å—Å–∏–≤ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ —Å–∫–ª–µ–∏–≤–∞–µ–º –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –∞ –∑–∞—Ç–µ–º —Ä–∞–∑–±–∏–≤–∞–µ–º 
    # –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–∂–µ—Å—Ç–≤–∞. –ü–æ–ª—É—á–∏–≤—à–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã —Ç–∞–∫ –∂–µ 
    # –ø—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ —Ç–æ–∫–µ–Ω–∞–∑–µ—Ä –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤.
    unk_chars = set()
    for char in set("".join(unk_words)):
        unk_chars.update(get_unk_words(char))

    return sorted(list(unk_chars))


class DataCollatorWithPaddingPlusLabels(DataCollatorWithPadding):
    def __call__(self, features):
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–∫ (labels)
        labels = [feature.pop('labels') for feature in features]
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –∫–æ–ª–ª–∞—Ç–æ—Ä–∞ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        batch = super().__call__(features)
        # –ü–∞–¥–¥–∏–Ω–≥ –¥–ª—è labels —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º -100
        batch['labels'] = torch.nn.utils.rnn.pad_sequence(
            labels, batch_first=True, padding_value=-100
        )
        return batch


class DataCollatorWithRandomMasking(DataCollatorWithPadding):
    def __init__(self, tokenizer, prob=.15, **kwargs):
        super().__init__(tokenizer, **kwargs)
        self.prob = prob

    def __call__(self, features):
        # –ü–∞–¥–¥–∏–Ω–≥ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞–∑–æ–≤–æ–≥–æ collator
        batch = super().__call__(features)

        input_ids = batch['input_ids']

        special_tokens_mask = [
            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)
            for val in input_ids
        ]
        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)
        
        probability_matrix = torch.zeros(input_ids.shape).masked_fill(~special_tokens_mask, value=self.prob)
        masked_indices = torch.bernoulli(probability_matrix).bool()

        # 95% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])
        indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.95)).bool() & masked_indices
        batch['input_ids'][indices_replaced] = self.tokenizer.mask_token_id

        # 5% of the time, we replace masked input tokens with random word
        indices_random = torch.bernoulli(torch.full(input_ids.shape, 1.)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(len(self.tokenizer), input_ids.shape, dtype=torch.long)
        batch['input_ids'][indices_random] = random_words[indices_random]
    
        return batch